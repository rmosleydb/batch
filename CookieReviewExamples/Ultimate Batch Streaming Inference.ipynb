{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b2f6bd7-215c-473a-bff9-1a8c53da23f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14476fdc-640c-4c64-8b92-1a332eb0bf2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Batch Inference\n",
    "\n",
    " - This notebook is designed to take rows from input_table_name, and use ai_query to create results in output_table_name.\n",
    " - You can keep adding rows to input_table_name, and run this notebook. The new rows will be appended (with results) into output_table_name.\n",
    " - The notebook uses structured streaming to create batches of approximately inference_rows_per_batch. This allows recovery from unexpected failure (for example of the endpoint). Just restart the job.\n",
    " - Each row will be processed exactly once. [Currently, there is a very small chance of one batch being reprocessed if the job fails. This will be fixed shortly.]\n",
    "\n",
    "#### To use\n",
    "\n",
    " - Specify all the non-optional parameters and run the notebook.\n",
    " - You can schedule this notebook as a job. Specify all the parameters required to the job and trigger the job when new data is required to be processed from input_table_name.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    " - There are two streams.\n",
    " - Stream1\n",
    "   - Copies (input_table_pk, input_column_name) into an intermediate table (in tmp_schema), with partitions (or files) with approximately rows_per_batch. This is requied to do the actual ai_query processing in batches of rows_per_batch.\n",
    " - Stream2\n",
    "   - Creates the smallest batch size possible (one batch per file in the intermediate table), calls ai_query and appends the results into output_table_name.\n",
    " - Structured streaming is resilient to crashes, as its checkpoint and write to Delta are transactional. So just restarting the job if it crashes (for a transient error) will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "983791ad-2ce6-4700-a8ba-883651f2786f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is required for provisioning_utils (see below)\n",
    "try:\n",
    "    from mlflow.utils.databricks_utils import get_databricks_host_creds, get_workspace_url\n",
    "    print(\"MLflow utilities imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(\"Failed to import MLflow utilities. Installing MLflow...\")\n",
    "    # Install MLflow\n",
    "    %pip install mlflow\n",
    "    # Restart Python to ensure the newly installed library is loaded\n",
    "    dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c267783-4f06-407e-b59c-59de12528b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import math\n",
    "import datetime, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1ed31f0-0ccb-40da-9613-505a0477728d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b68bca6-3003-456d-8075-bd6101bc4733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1cfcf46-4c61-4c92-bb57-b5c2f9f14536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, math, os, time, uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, rand\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "# Set up widgets\n",
    "\n",
    "# Widgets for input and output data.\n",
    "dbutils.widgets.text(\"input_table_name\", \"\", \"01. Input Table Name\")\n",
    "dbutils.widgets.text(\"input_column_name\", \"\", \"02. Input Column Name\")\n",
    "dbutils.widgets.text(\"input_table_pk\", \"\", \"03. (optional) Input Table Primary Key (comma-separated if composite)\")\n",
    "dbutils.widgets.text(\"output_table_name\", \"\", \"04. Output Table Name\")\n",
    "dbutils.widgets.text(\"output_column_name\", \"results\", \"05. Output Column Name\")\n",
    "dbutils.widgets.text(\"tmp_schema\", \"\", \"06. Temp Schema for temp tables\")\n",
    "\n",
    "# Widgets for model parameters\n",
    "dbutils.widgets.text(\"endpoint_name\", \"\", \"07. (Optional) Endpoint Name\")\n",
    "dbutils.widgets.dropdown(\"use_existing_endpoint\", \"false\", [\"true\", \"false\"], \"071. Use existing endpoint (see instructions)\")\n",
    "dbutils.widgets.text(\"model_uc_path\", \"\", \"08. Model UC Path\")\n",
    "dbutils.widgets.text(\"model_uc_version\", \"\", \"09. (optional) Model Version\")\n",
    "dbutils.widgets.text(\"ptus\", \"\", \"10. Provisioned Throughput Units\")\n",
    "dbutils.widgets.text(\"prompt\", \"\", \"11. Prompt\")\n",
    "dbutils.widgets.text(\"checkpoint_base_path\", \"\", \"12. Base path for stream checkpoint\")\n",
    "dbutils.widgets.text(\"model_param_max_tokens\", \"\", \"13. (optional) Model Param: Max Tokens\")\n",
    "dbutils.widgets.text(\"model_param_temperature\", \"0\", \"14. (optional) Model Param: Temperature\")\n",
    "\n",
    "# Widgets for batch inference\n",
    "dbutils.widgets.text(\"inference_processing_rate_rows_per_second\", \"\", \"15. (Optional) Inference Processing Rate: Rows per Second per PTU\")  # Derived from experimentation\n",
    "dbutils.widgets.text(\"inference_rows_per_batch\", \"\", \"16. Rows per streaming mini batch\")  # Approximate rows per streaming batch for inference\n",
    "\n",
    "dbutils.widgets.text(\"repartition_stream\", \"\", \"17. (optional) Number of repartitions to the stream\") \n",
    "dbutils.widgets.dropdown(\"is_debug\",  \"false\", [\"true\", \"false\"], \"18. Is Debug\")  # When scheduled, set this to false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d21ced5-7def-47b8-b330-ea507a633324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params_dict = dbutils.widgets.getAll()\n",
    "json.dumps(params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "728adb8e-8f43-4edd-8d72-bec06d47380d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# TODO: Work out how to get this in a debug environment because I need to remove this cell\n",
    "if params_dict.get(\"is_debug\", \"false\").lower() == \"true\":\n",
    "  print(\"DEBUG: Overriding with debug parameters\")\n",
    "  with open('test_batch_parameters.json', 'r') as f:\n",
    "    params_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "027e700a-ebfa-4bd9-a3e3-82dcc850ef76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "params = AttributeDict(params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6d0e89a-57ce-4fcc-b982-c6b3a47e0cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Pre-parsing Parameters:\")\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f0fa08c-06a0-41d3-aa25-88f532bcd09f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate parameters\n",
    "\n",
    "# Data\n",
    "assert params.input_table_name.count(\".\") == 2, \"input_table_name should be in form catalog.schema.table\"\n",
    "assert params.input_column_name, \"input_column_name can not be empty\"\n",
    "\n",
    "input_table_pk = []\n",
    "if params.input_table_pk:\n",
    "    input_table_pk = [e.strip() for e in params.input_table_pk.split(',')]\n",
    "assert params.output_table_name.count(\".\") == 2, \"output_table_name should be in form catalog.schema.table\"\n",
    "assert params.output_column_name, \"output_column_name can not be empty\"\n",
    "assert params.tmp_schema.count(\".\") == 1, \"tmp_schema should be in form catalog.schema\"\n",
    "# TODO: check if tmp_schema exists\n",
    "assert params.checkpoint_base_path, \"checkpoint_base_path can not be empty and ideally should be exclusively for this batch inference notebook\"\n",
    "# TODO: check checkpoint_base_path exists\n",
    "\n",
    "# Model\n",
    "if params.endpoint_name:\n",
    "    assert params.endpoint_name.replace('-', '').replace('_', '').isalnum(), \"Endpoint name must be alphanumeric with hyphens and underscores allowed in between.\"\n",
    "params.use_existing_endpoint = str_to_bool(params.use_existing_endpoint, default_value='false')\n",
    "if params.use_existing_endpoint:\n",
    "    assert params.endpoint_name, \"endpoint_name can not be empty if use_existing_endpoint is set to true\"\n",
    "assert params.model_uc_path.count(\".\") == 2, \"model_uc_path should be in form catalog.schema.model_name\"\n",
    "if params.model_uc_version:\n",
    "    assert params.model_uc_version.isdigit(), \"model_uc_version should be a number\"\n",
    "    params.model_uc_version = int(params.model_uc_version)\n",
    "    assert params.model_uc_version >= 1, \"model_uc_version should be a positive integer\"\n",
    "params.ptus = int(params.ptus)\n",
    "assert params.ptus > 0, \"ptus should be a positive integer\"\n",
    "if not params.prompt:\n",
    "  print(\"WARNING: There is no prompt. This means your input column should have a prompt.\")\n",
    "\n",
    "if params.model_param_max_tokens:\n",
    "    assert params.model_param_max_tokens.isdigit(), \"model_param_max_tokens should be a number\"\n",
    "    params.model_param_max_tokens = int(params.model_param_max_tokens)\n",
    "    assert params.model_param_max_tokens >= 1, \"model_param_max_tokens should be a positive integer\"\n",
    "\n",
    "if params.model_param_temperature:\n",
    "    try:\n",
    "        params.model_param_temperature = float(params.model_param_temperature)\n",
    "    except ValueError:\n",
    "        raise AssertionError(\"model_param_temperature should be a float\")\n",
    "    assert params.model_param_temperature >= 0, \"model_param_temperature should be between 0 and 1\"\n",
    "    assert params.model_param_temperature < 1, \"model_param_temperature should be between 0 and 1\"\n",
    "\n",
    "if params.inference_processing_rate_rows_per_second:\n",
    "    assert params.inference_processing_rate_rows_per_second.isdigit(), \"inference_processing_rate_rows_per_second should be a number\"\n",
    "    params.inference_processing_rate_rows_per_second = int(params.inference_processing_rate_rows_per_second)\n",
    "    assert params.inference_processing_rate_rows_per_second > 0, \"inference_processing_rate_rows_per_second should be a positive number\"\n",
    "\n",
    "assert params.inference_rows_per_batch.isdigit(), \"inference_rows_per_batch should be a number\"\n",
    "params.inference_rows_per_batch = int(params.inference_rows_per_batch)\n",
    "assert params.inference_rows_per_batch > 0, \"inference_rows_per_batch should be a number\"\n",
    "\n",
    "if params.repartition_stream:\n",
    "    params.repartition_stream = int(params.repartition_stream)\n",
    "    assert params.repartition_stream > 0, \"repartition_stream should be a number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3670a71-6ecf-4145-bc8f-7ab62490eecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Post-parsing Parameters:\")\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74124c6-f570-4896-8441-4482a31787f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Get parameters\n",
    "# print(\"Post-parsing Parameters:\")\n",
    "# print(\"----------\")\n",
    "# print(\"\\nData parameters:\")\n",
    "# print(f\"input_table_name: {input_table_name}\")\n",
    "# print(f\"input_column_name: {input_column_name}\")\n",
    "# print(f\"output_table_name: {output_table_name}\")\n",
    "# print(f\"output_column_name: {output_column_name}\")\n",
    "# print(f\"tmp_schema: {tmp_schema}\")\n",
    "\n",
    "# print(\"\\nModel parameters:\")\n",
    "# print(f\"endpoint_name: {endpoint_name}\")\n",
    "# print(f\"model_uc_path: {model_uc_path}\")\n",
    "# print(f\"prompt: {prompt}\")\n",
    "# print(f\"model_uc_version: {model_uc_version}\")\n",
    "# print(f\"max_tokens: {max_tokens}\")\n",
    "# print(f\"temperature: {temperature}\")\n",
    "# print(f\"ptus: {ptus}\")\n",
    "# print(f\"checkpoint_base_path: {checkpoint_base_path}\")\n",
    "# print(f\"max_tokens: {max_tokens}\")\n",
    "# print(f\"temperature: {temperature}\")\n",
    "\n",
    "# print(\"\\nBatch inference parameters:\")\n",
    "# print(f\"inference_processing_rate_rows_per_second: {inference_processing_rate_rows_per_second}\")\n",
    "# print(f\"inference_rows_per_batch: {inference_rows_per_batch}\")\n",
    "# print(\"----------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1393aac3-a057-44db-9093-8deaab990d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if params.inference_processing_rate_rows_per_second:\n",
    "  expected_time_per_batch_seconds = params.inference_rows_per_batch / (params.inference_processing_rate_rows_per_second * params.ptus)\n",
    "  print(f\"expected_time_per_batch_seconds: {expected_time_per_batch_seconds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4688073f-e225-4514-87b1-a11e35f65e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_source_size_rows = spark.sql(f\"SELECT COUNT(1) FROM {params.input_table_name}\").collect()[0][0]\n",
    "print(f\"total_source_size_rows: {total_source_size_rows}\")\n",
    "\n",
    "total_target_size_rows = 0\n",
    "if spark.catalog.tableExists(params.output_table_name):\n",
    "  total_target_size_rows = spark.sql(f\"SELECT COUNT(1) FROM {params.output_table_name}\").collect()[0][0]\n",
    "print(f\"total_target_size_rows: {total_target_size_rows}\")\n",
    "\n",
    "new_rows_to_process = total_source_size_rows - total_target_size_rows\n",
    "print(f\"new_rows_to_process: {new_rows_to_process}\")\n",
    "\n",
    "if params.inference_processing_rate_rows_per_second:\n",
    "  expected_time_for_workload_seconds = new_rows_to_process / (params.inference_processing_rate_rows_per_second * params.ptus)\n",
    "  print(f\"expected_time_for_workload_seconds: {expected_time_for_workload_seconds} ({expected_time_for_workload_seconds / 60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de4ed8b0-9687-4b04-979b-c719c4012fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_partitions = math.ceil(new_rows_to_process / params.inference_rows_per_batch)\n",
    "print(f\"num_partitions: {num_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f52e1721-4575-49af-bb85-b230f42ff956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp_input_table should only depend on the source and target so it is deterministic and the same across restarts of the job.\n",
    "tmp_input_table = f'{params.tmp_schema}.tmp_{params.input_table_name.replace(\".\", \"_\")}_{params.output_table_name.replace(\".\", \"_\")}'\n",
    "print(tmp_input_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bca6665-a271-400f-811c-2ee0c641b9eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Turn off optimized writes to create small files\n",
    "# Only create the table if it does not exist.\n",
    "# If it already exists, then do nothing. It should already have the data from a previous run, so just use it.\n",
    "select_str = \", \".join([e for e in (*input_table_pk, params.input_column_name, \"current_timestamp as ctime\")])\n",
    "sql_str = f\"\"\"\n",
    "create table if not exists {tmp_input_table}\n",
    "tblproperties('delta.autoOptimize.autoCompact' = false, 'delta.autoOptimize.optimizeWrite' = false)\n",
    "as\n",
    "-- select {params.input_table_pk}, {params.input_column_name}, current_timestamp as ctime\n",
    "select {select_str}\n",
    "from {params.input_table_name}\n",
    "limit 0;\n",
    "\"\"\"\n",
    "print(sql_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f6fd17e-a0fd-4956-a0b6-7e4da4235e76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(sql_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bea8b30-a7f6-451d-b42e-312a9a782468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I do not want this table to be optimized into larger files.\n",
    "sql_str = f\"alter table {tmp_input_table} disable predictive optimization\"\n",
    "print(sql_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "190ff238-6243-4796-86d8-43b753d8a1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "try:\n",
    "  spark.sql(sql_str)\n",
    "except:\n",
    "  traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f8b611a-f65b-4b90-ae3f-f66534f4856b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_with_partitions(df, batch_id):\n",
    "  num_rows = df.count()\n",
    "  num_partitions = math.ceil(num_rows / params.inference_rows_per_batch)\n",
    "  df.repartition(num_partitions).write.mode(\"append\").saveAsTable(tmp_input_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7013f9d-b893-4e9b-8b91-0388104845b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp_input_table uses the source and target tables, so if any one of those changes, this checkpoint will change.\n",
    "# Always just use the source and destination of *this* stream as the checkpoint.\n",
    "stream1_checkpoint_path = f\"{params.checkpoint_base_path}/{params.input_table_name.lower().replace('.', '_')}/{tmp_input_table.replace('.', '_')}\"\n",
    "print(stream1_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10342284-4443-4b5f-92ad-d292fbd44c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream1_select_expr = tuple([*input_table_pk, params.input_column_name, \"current_timestamp as ctime\"])\n",
    "print(stream1_select_expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "589d0864-a4f3-40f2-a9a1-bc5e8501f0e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream1_df = (\n",
    "  spark.readStream\n",
    "  #.option(\"maxBytesPerTrigger\", 1)\n",
    "  .table(params.input_table_name)\n",
    "  .selectExpr(*stream1_select_expr)\n",
    "  .writeStream\n",
    "  .trigger(availableNow=True)\n",
    "  .option(\"checkpointLocation\", stream1_checkpoint_path)\n",
    "  .foreachBatch(write_with_partitions)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d865f2-71ef-4c3f-aa9c-abd27f6b46d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream1_df.start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "672653fc-b1c2-485f-846b-9b60046444d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "provisioning_utils = ProvisioningUtils(use_existing_endpoint=params.use_existing_endpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b592865-28df-446c-949f-08c8c16270b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_endpoint_name = params.endpoint_name\n",
    "if not model_endpoint_name:\n",
    "  ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "  model_endpoint_name = f\"batch_inference_{ts}\"\n",
    "print(f\"model_endpoint_name: {model_endpoint_name}\")\n",
    "\n",
    "provisioning_utils.create_endpoint(name=model_endpoint_name, model_name=params.model_uc_path, ptu=params.ptus, workload_size=\"Large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f06f748-2bd2-42bd-b0ee-e1f174c973a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "provisioning_utils.monitor_endpoint(name=model_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca35fbd9-31de-44c4-b588-204f3b350a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_params_str = f\"named_struct('temperature', {params.model_param_temperature})\"\n",
    "if params.model_param_max_tokens:\n",
    "  model_params_str = f\"named_struct('max_tokens', {params.model_param_max_tokens}, 'temperature', {params.model_param_temperature})\"\n",
    "ai_query_expr = f\"\"\"ai_query('{model_endpoint_name}', CONCAT('{params.prompt}', {params.input_column_name}), modelParameters => {model_params_str}) as {params.output_column_name}\"\"\"\n",
    "print(ai_query_expr)\n",
    "\n",
    "select_expr = tuple([*input_table_pk, params.input_column_name, ai_query_expr, \"current_timestamp as ctime\"])\n",
    "#select_expr = f'\"{input_table_pk}\", \"{input_column_name}\", \"{ai_query_expr}\"'\n",
    "print(select_expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb37b4e2-053a-4ff6-85ce-efe95461ee6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = f\"{params.checkpoint_base_path}/{tmp_input_table.replace('.', '_')}/{params.output_table_name.lower().replace('.', '_')}\"\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef0c3fc2-11e0-4637-ae18-c72e16a9799d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_stream_df = (\n",
    "  spark.readStream\n",
    "  .option(\"maxBytesPerTrigger\", 1)\n",
    "  .table(tmp_input_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af14245e-9dc2-4c60-b81d-c0c108802737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if params.repartition_stream and params.repartition_stream > 0:\n",
    "  print(f\"repartitioning stream [{params.repartition_stream}]\")\n",
    "  read_stream_df = read_stream_df.repartition(params.repartition_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7a7201f-de84-4f07-9587-9d73567857b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_stream_df =  read_stream_df.selectExpr(*select_expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ae3ea33-e2d3-4d23-b74a-e03abe335b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_start = time.time()\n",
    "print(f\"stream_start: {stream_start}\")\n",
    "\n",
    "write_stream_df = (\n",
    "  read_stream_df\n",
    "  .writeStream\n",
    "  .trigger(availableNow=True)\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .outputMode(\"append\")\n",
    "  .option(\"mergeSchema\", \"true\")\n",
    "  .toTable(params.output_table_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61a5a662-a0dd-415a-aaa4-c3d8635bc9ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "write_stream_df.awaitTermination()\n",
    "stream_end = time.time()\n",
    "print(f\"stream_end: {stream_end}\")\n",
    "\n",
    "print(f\"stream_duration_seconds: {stream_end - stream_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d625b2a-6fae-4a84-803e-caf852d33af4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if (not params.endpoint_name \n",
    "    or not params.use_existing_endpoint):\n",
    "  print(f\"stopping endpoint {model_endpoint_name}\")\n",
    "  r = provisioning_utils.stop_endpoint(name=model_endpoint_name)\n",
    "  print(r)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Ultimate Batch Streaming Inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
